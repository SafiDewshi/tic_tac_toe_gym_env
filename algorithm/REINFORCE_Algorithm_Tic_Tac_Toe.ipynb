{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "REINFORCE Algorithm Tic-Tac-Toe.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HM8ZWljqgbY",
        "colab_type": "text"
      },
      "source": [
        "# 1.0 Install gym environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HFYXNRKPydY",
        "colab_type": "text"
      },
      "source": [
        "Run this cell then restart the runtime\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChuC05fX9bhq",
        "colab_type": "code",
        "outputId": "b79af32a-050d-4b13-ee1b-ace231ab25f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "source": [
        "!git clone https://github.com/SafiDewshi/tic_tac_toe_gym_env.git\n",
        "!pip install -e tic_tac_toe_gym_env"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'tic_tac_toe_gym_env'...\n",
            "remote: Enumerating objects: 92, done.\u001b[K\n",
            "remote: Counting objects: 100% (92/92), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 92 (delta 28), reused 86 (delta 23), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (92/92), done.\n",
            "Obtaining file:///content/tic_tac_toe_gym_env\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from gym-tictactoe==0.0.1) (0.17.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gym-tictactoe==0.0.1) (1.18.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-tictactoe==0.0.1) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-tictactoe==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->gym-tictactoe==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->gym-tictactoe==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->gym-tictactoe==0.0.1) (0.16.0)\n",
            "Installing collected packages: gym-tictactoe\n",
            "  Running setup.py develop for gym-tictactoe\n",
            "Successfully installed gym-tictactoe\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOQbSj8DqmXM",
        "colab_type": "text"
      },
      "source": [
        "# 2.0 Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oek7ct1rJsNJ",
        "colab_type": "code",
        "outputId": "de3cbd00-b8b0-46c6-eebf-1a5f4b5de269",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import gym\n",
        "import gym_tictactoe\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibtvGuz3qrLQ",
        "colab_type": "text"
      },
      "source": [
        "# 3.0 REINFORCE Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xUl9_bXHuAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_one_step(env, obs, model, loss_fn):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Call the model, give it single observation, outputs probability of playing each square\n",
        "        prob = model(obs.reshape(9)[np.newaxis])\n",
        "\n",
        "        # find the max probability spot\n",
        "        max_prob = tf.math.reduce_max(prob)\n",
        "        \n",
        "        take_best = (max_prob > tf.random.uniform([1,1]))\n",
        "\n",
        "        if take_best:\n",
        "          x = tf.argmax(tf.reshape(tf.squeeze(prob), 9))\n",
        "          action = (x//3, x%3)\n",
        "        else:\n",
        "          mov = env.observation_space.sample()\n",
        "          action = (mov[0], mov[1])\n",
        "\n",
        "        mean_prob = tf.math.reduce_mean(prob)\n",
        "\n",
        "        confidence = tf.math.subtract(max_prob, mean_prob)\n",
        "        \n",
        "        # find the difference between the highest probability and the lowest \n",
        "        loss = tf.reduce_mean(loss_fn(tf.constant([[1.]]), confidence))\n",
        "\n",
        "        # the higher the difference between the most confident play and \n",
        "        # the second most confident play, the better\n",
        "    \n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    #find the gradients that reduce that loss\n",
        "    obs, reward, done = env.step(action)\n",
        "    \n",
        "    return obs, reward, done, grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvpSn1MQJI7d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_multiple_episodes(env, n_episodes, n_max_steps, model1, model2, loss_fn):\n",
        "    model1_all_rewards = []\n",
        "    model1_all_grads = []\n",
        "    model2_all_rewards = []\n",
        "    model2_all_grads = []\n",
        "\n",
        "    # Plays multiple episodes\n",
        "    for episode in range(n_episodes):\n",
        "        model1_current_rewards = []\n",
        "        model1_current_grads = []\n",
        "        model2_current_rewards = []\n",
        "        model2_current_grads = []\n",
        "        obs = env.reset()\n",
        "\n",
        "        # Plays an episode\n",
        "        for step in range(n_max_steps):\n",
        "\n",
        "            obs, reward, done, grads = play_one_step(env, obs, model1, loss_fn)\n",
        "            model1_current_rewards.append(reward)\n",
        "            model1_current_grads.append(grads)\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            obs, reward, done, grads = play_one_step(env, obs, model2, loss_fn)\n",
        "            model2_current_rewards.append(reward)\n",
        "            model2_current_grads.append(grads)\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        model1_all_rewards.append(model1_current_rewards)\n",
        "        model1_all_grads.append(model1_current_grads)\n",
        "\n",
        "        model2_all_rewards.append(model2_current_rewards)\n",
        "        model2_all_grads.append(model2_current_grads)\n",
        "        \n",
        "    # Returns list of reward lists (one per episode, containing one reward per step) and\n",
        "    # a list of gradient lists (one per episode, one tuple of gradients per step, each tuple containing one gradient\n",
        "    # tensor per trainable variable)\n",
        "    return model1_all_rewards, model1_all_grads, model2_all_rewards, model2_all_grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qtenWq1JJoe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Computes the discounted rewards\n",
        "def discount_rewards(rewards, discount_rate):\n",
        "    discounted = np.array(rewards)\n",
        "    for step in range(len(rewards) - 2, -1, -1):\n",
        "        discounted[step] += discounted[step + 1] * discount_rate\n",
        "    return discounted\n",
        "\n",
        "# Discounts and normalizes rewards\n",
        "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
        "    all_discounted_rewards = [discount_rewards(rewards, discount_rate) for rewards in all_rewards] \n",
        "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
        "    reward_mean = flat_rewards.mean()\n",
        "    reward_std = flat_rewards.std()\n",
        "    return [(discounted_rewards - reward_mean) / reward_std for discounted_rewards in all_discounted_rewards]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7RgGPpRqy9s",
        "colab_type": "text"
      },
      "source": [
        "# 4.0 Train Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWPdWtJVJdjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "n_iterations = 10000\n",
        "n_episodes_per_update = 10\n",
        "n_max_steps = 10\n",
        "discount_rate = 0.95\n",
        "optimizer = keras.optimizers.Adam(lr=0.01)\n",
        "loss_fn = keras.losses.binary_crossentropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJbUSqSSDjyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model():\n",
        "  \"\"\"model = Sequential()\n",
        "  model.add(Dense(72, activation='relu', input_shape=[9]))\n",
        "  model.add(Dense(36, activation='relu'))\n",
        "  model.add(Dense(18, activation='relu'))\n",
        "  model.add(Dense(9, activation = 'sigmoid'))\"\"\"\n",
        "\n",
        "  model = keras.models.Sequential([\n",
        "    keras.layers.Dense(72, activation='relu', input_shape=[9]),\n",
        "    keras.layers.Dense(36, activation='relu'),\n",
        "    keras.layers.Dense(18, activation='relu'),\n",
        "    keras.layers.Dense(9, activation='sigmoid'),                             \n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yu3A7RjwJgWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "model1 = model()\n",
        "model2 = model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fZj6nyiJioE",
        "colab_type": "code",
        "outputId": "c25c331e-d627-4ca2-dcae-0708e0ee3ad5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "\n",
        "  env = gym.make(\"tictactoe-v0\")\n",
        "\n",
        "  for iteration in range(n_iterations):\n",
        "    \n",
        "      # Plays the game n times and returns the rewards and gradients for every episode and step\n",
        "      model1_all_rewards, model1_all_grads, model2_all_rewards, model2_all_grads = play_multiple_episodes(env, n_episodes_per_update, n_max_steps, model1, model2, loss_fn)\n",
        "      \n",
        "      model1_total_rewards = sum(map(sum, model1_all_rewards))\n",
        "      model2_total_rewards = sum(map(sum, model2_all_rewards))\n",
        "      model1_mean = model1_total_rewards / n_episodes_per_update\n",
        "      model2_mean = model2_total_rewards / n_episodes_per_update\n",
        "      print(\"\\rIteration: {}, model 1 mean rewards: {:.1f}, model 2 mean rewards: {:.1f}, sum of both:{:.1f}\".format(iteration, model1_mean, model2_mean, model1_mean+model2_mean), end=\"\")\n",
        "      \n",
        "      # Computes each action's normalized advantage, provides measure of how good each action was\n",
        "      model1_all_final_rewards = discount_and_normalize_rewards(model1_all_rewards, discount_rate)\n",
        "      model2_all_final_rewards = discount_and_normalize_rewards(model2_all_rewards, discount_rate)\n",
        "\n",
        "\n",
        "      model1_all_mean_grads = []\n",
        "      model2_all_mean_grads = []\n",
        "      for var_index in range(len(model1.trainable_variables)):\n",
        "\n",
        "          model1_mean_grads = tf.reduce_mean([model1_final_reward * model1_all_grads [episode_index][step][var_index] \n",
        "                                      for episode_index, model1_final_rewards in enumerate(model1_all_final_rewards)\n",
        "                                      for step, model1_final_reward in enumerate(model1_final_rewards)], axis=0)\n",
        "          model1_all_mean_grads.append(model1_mean_grads)\n",
        "\n",
        "          model2_mean_grads = tf.reduce_mean([model2_final_reward * model2_all_grads [episode_index][step][var_index] \n",
        "                                      for episode_index, model2_final_rewards in enumerate(model2_all_final_rewards)\n",
        "                                      for step, model2_final_reward in enumerate(model2_final_rewards)], axis=0)\n",
        "          model2_all_mean_grads.append(model2_mean_grads)\n",
        "\n",
        "      optimizer.apply_gradients(zip(model1_all_mean_grads, model1.trainable_variables))\n",
        "      optimizer.apply_gradients(zip(model2_all_mean_grads, model2.trainable_variables))\n",
        "      \n",
        "  env.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 73, model 1 mean rewards: -18.0, model 2 mean rewards: 2.0, sum of both:-16.0"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: 9999, model 1 mean rewards: -6.7, model 2 mean rewards: -6.1, sum of both:-12.8"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-EE6coorE2H",
        "colab_type": "text"
      },
      "source": [
        "# 5.0 Play Tic-Tac-Toe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b6hx4eJGH9k",
        "colab_type": "code",
        "outputId": "542896d1-cac5-4a87-9f93-c7af9f02dbdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "obs = env.reset()\n",
        "obs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxaUNexXVAWk",
        "colab_type": "code",
        "outputId": "75b3b82a-365a-40ab-8dba-743d26c5201d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "prob = model2(obs.reshape(9)[np.newaxis])\n",
        "max_prob = tf.math.reduce_max(prob)\n",
        "x = tf.argmax(tf.reshape(tf.squeeze(prob), 9))\n",
        "action = (x//3, x%3)\n",
        "obs, reward, done = env.step(action)\n",
        "print(prob, obs, reward, done)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[nan nan nan nan nan nan nan nan nan]], shape=(1, 9), dtype=float32) [[1. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]] -20 True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKzIVqmeMu8j",
        "colab_type": "code",
        "outputId": "67f338e3-be69-4c48-8a4f-3056cb6d1af4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "prob = model1(obs.reshape(9)[np.newaxis])\n",
        "max_prob = tf.math.reduce_max(prob)\n",
        "x = tf.argmax(tf.reshape(tf.squeeze(prob), 9))\n",
        "action = (x//3, x%3)\n",
        "obs, reward, done = env.step(action)\n",
        "print(prob, obs, reward, done)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[nan nan nan nan nan nan nan nan nan]], shape=(1, 9), dtype=float32) [[1. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]] 1 False\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}