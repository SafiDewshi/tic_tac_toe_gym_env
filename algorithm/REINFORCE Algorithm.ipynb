{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"REINFORCE Algorithm.ipynb","provenance":[],"authorship_tag":"ABX9TyO8CHGGMZkq1VjbumjtBICo"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"oek7ct1rJsNJ","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","mpl.rc('animation', html='jshtml')\n","import gym"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6xUl9_bXHuAA","colab_type":"code","colab":{}},"source":["def play_one_step(env, obs, model, loss_fn):\n","    with tf.GradientTape() as tape:\n","        # Call the model, give it single observation, outputs probability of going left\n","        left_proba = model(obs[np.newaxis])\n","        \n","        # Sample random float between 0 and 1, check if greater than left_proba\n","        action = (tf.random.uniform([1,1]) > left_proba)\n","        \n","        # Define target probability of going left\n","        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n","        \n","        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n","    \n","    grads = tape.gradient(loss, model.trainable_variables)\n","    obs, reward, done, info = env.step(int(action[0,0].numpy()))\n","    return obs, reward, done, grads"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DvpSn1MQJI7d","colab_type":"code","colab":{}},"source":["def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n","    all_rewards = []\n","    all_grads = []\n","    # Plays multiple episodes\n","    for episode in range(n_episodes):\n","        current_rewards = []\n","        current_grads = []\n","        obs = env.reset()\n","        # Plays an episode\n","        for step in range(n_max_steps):\n","            obs, reward, done, grads = play_one_step(env, obs, model, loss_fn)\n","            current_rewards.append(reward)\n","            current_grads.append(grads)\n","            if done:\n","                break\n","        all_rewards.append(current_rewards)\n","        all_grads.append(current_grads)\n","    # Returns list of reward lists (one per episode, containing one reward per step) and\n","    # a list of gradient lists (one per episode, one tuple of gradients per step, each tuple containing one gradient\n","    # tensor per trainable variable)\n","    return all_rewards, all_grads"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9qtenWq1JJoe","colab_type":"code","colab":{}},"source":["# Computes the discounted rewards\n","def discount_rewards(rewards, discount_rate):\n","    discounted = np.array(rewards)\n","    for step in range(len(rewards) - 2, -1, -1):\n","        discounted[step] += discounted[step + 1] * discount_rate\n","    return discounted\n","\n","# Discounts and normalizes rewards\n","def discount_and_normalize_rewards(all_rewards, discount_rate):\n","    all_discounted_rewards = [discount_rewards(rewards, discount_rate) for rewards in all_rewards] \n","    flat_rewards = np.concatenate(all_discounted_rewards)\n","    reward_mean = flat_rewards.mean()\n","    reward_std = flat_rewards.std()\n","    return [(discounted_rewards - reward_mean) / reward_std for discounted_rewards in all_discounted_rewards]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lSVZzGf3JXZS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"1473a3b4-8a31-464d-aae3-01c136fff0f7","executionInfo":{"status":"ok","timestamp":1587395656467,"user_tz":-60,"elapsed":689,"user":{"displayName":"George Walter","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjH4yaUckba0d7mAEwbLW697_Bu_aNsy197XeT4uA=s64","userId":"05600638246641531936"}}},"source":["discount_rewards([10, 0, -50], discount_rate=0.8)"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-22, -40, -50])"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"o51uB0YsJcU-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"d8647c13-1715-4c98-e0f4-f7db3cd34f7a","executionInfo":{"status":"ok","timestamp":1587395658202,"user_tz":-60,"elapsed":688,"user":{"displayName":"George Walter","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjH4yaUckba0d7mAEwbLW697_Bu_aNsy197XeT4uA=s64","userId":"05600638246641531936"}}},"source":["# Positive normalized action advantages are good whereas negative normalized action advantages are bad\n","discount_and_normalize_rewards([[10,0,-50], [10,20]], discount_rate=0.8)"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([-0.28435071, -0.86597718, -1.18910299]),\n"," array([1.26665318, 1.0727777 ])]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"yWPdWtJVJdjt","colab_type":"code","colab":{}},"source":["# Hyperparameters\n","n_iterations = 150\n","n_episodes_per_update = 10\n","n_max_steps = 200\n","discount_rate = 0.95\n","optimizer = keras.optimizers.Adam(lr=0.01)\n","loss_fn = keras.losses.binary_crossentropy"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yu3A7RjwJgWa","colab_type":"code","colab":{}},"source":["keras.backend.clear_session()\n","tf.random.set_seed(42)\n","np.random.seed(42)\n","\n","n_inputs = 4 # == env.observation_space.shape[0]\n","\n","model = keras.models.Sequential([\n","    keras.layers.Dense(5, activation=\"elu\", input_shape=[n_inputs]),\n","    keras.layers.Dense(1, activation=\"sigmoid\"),\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_fZj6nyiJioE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"fb583a62-0c76-4c18-fdb0-8c0e006b0ccd"},"source":["with tf.device('/device:GPU:0'):\n","\n","  env = gym.make(\"CartPole-v1\")\n","  env.seed(42)\n","\n","  for iteration in range(n_iterations):\n","      # Plays the game n times and returns the rewards and gradients for every episode and step\n","      all_rewards, all_grads = play_multiple_episodes(env, n_episodes_per_update, n_max_steps,model, loss_fn)\n","      \n","      total_rewards = sum(map(sum, all_rewards))                     \n","      print(\"\\rIteration: {}, mean rewards: {:.1f}\".format(iteration, total_rewards / n_episodes_per_update), end=\"\")\n","      \n","      # Computes each action's normalized advantage, provides measure of how good each action was\n","      all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_rate)\n","      \n","      all_mean_grads = []\n","      for var_index in range(len(model.trainable_variables)):\n","          mean_grads = tf.reduce_mean([final_reward * all_grads [episode_index][step][var_index] \n","                                      for episode_index, final_rewards in enumerate(all_final_rewards)\n","                                      for step, final_reward in enumerate(final_rewards)], axis=0)\n","          all_mean_grads.append(mean_grads)\n","      optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n","\n","  env.close()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Iteration: 10, mean rewards: 31.0"],"name":"stdout"}]}]}